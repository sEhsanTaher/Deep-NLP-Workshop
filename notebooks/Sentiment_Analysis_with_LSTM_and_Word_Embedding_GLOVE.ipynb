{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis with LSTM and Word Embedding GLOVE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mUJ5zzIZOYk6"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6LTwFNeQsDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7ysOubkQv_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6V9f_0bOVVS",
        "colab_type": "text"
      },
      "source": [
        "#KERAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3OAzDn3s8vR",
        "colab_type": "code",
        "outputId": "e3c89f5f-64a5-4020-e20e-a66996a1ab86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b531M-oYhn0Z",
        "colab_type": "text"
      },
      "source": [
        "## prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGyuqBxrhnjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"sentiment.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GsSlYzniKB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word2id(df):\n",
        "  vocab=set()\n",
        "  for i in df.iteritems():\n",
        "    words= i[1].split(' ')\n",
        "    vocab.update(words)\n",
        "  word2id= dict()\n",
        "  id2word= dict()\n",
        "  for i,word in enumerate(vocab):\n",
        "    word2id[word]=i+1\n",
        "    id2word[i+1] =word\n",
        "  return word2id,id2word,vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YMfFQvblsl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(df,word2id):\n",
        "  new_dataset=[]\n",
        "  for r in df.iteritems():\n",
        "    new_dataset.append([word2id[w] for w in r[1].split(\" \")])\n",
        "  return new_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNnjtLIhmoVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2id,id2word,vocab = get_word2id(df['sentence'])\n",
        "new_dataset = convert_data(df['sentence'],word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvFm1961r7Fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p23hik2QssbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_dataset = pad_sequences(new_dataset, maxlen=128, dtype='int32', padding='post', truncating='post', value=0.0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ztmpKZLs_Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_dataset = np.concatenate([new_dataset,df['polarity'].values.reshape(-1,1)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2kvcgLvz91N",
        "colab_type": "text"
      },
      "source": [
        "### split train , test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLh-4GCE0Ao-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = new_dataset[:int(len(new_dataset)*.8)]\n",
        "test = new_dataset[int(len(new_dataset)*.8):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swy7zrcMiNbw",
        "colab_type": "text"
      },
      "source": [
        "### evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4xKPRGq335J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(y_true,y_pred):\n",
        "  return confusion_matrix(y_true, y_pred, labels=[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-txStW8whlse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size,16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16,activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(16,activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(2,activation=tf.nn.softmax))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8dAvXvIzWSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x = train[:,:-1],y=train[:,-1],batch_size=32,epochs=5,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57_dnHTlIly",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis with LSTM and pre-trained word embeddings (Glove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzmP4jfws3cy",
        "colab_type": "text"
      },
      "source": [
        "### download and prepare embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZJjC1JIlIT2",
        "colab_type": "code",
        "outputId": "25226afd-c61f-4794-d145-b2f82667d73c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-23 22:42:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-12-23 22:42:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-12-23 22:42:36--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2        6%[>                   ]  52.84M  19.8MB/s               ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IUSuvrctBqp",
        "colab_type": "code",
        "outputId": "96a974c5-fc28-410c-aa02-d0820a0c9fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!unzip glove.6B.zip.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip.1\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEpv4NJFs7zF",
        "colab_type": "code",
        "outputId": "c43672f4-29e3-467e-97b1-bee6db85137f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "with open('glove.6B.50d.txt') as f:\n",
        "    rows, cols = 400000,50\n",
        "\n",
        "vectorsdf = pd.read_csv(\n",
        "    'glove.6B.50d.txt', sep=' ',  header=None, index_col=0,\n",
        "    quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "\n",
        "# remove one junk column\n",
        "vectorsdf = vectorsdf.dropna(axis=1)\n",
        "assert vectorsdf.shape == (int(rows), int(cols))\n",
        "vectorsdf.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.418000</td>\n",
              "      <td>0.249680</td>\n",
              "      <td>-0.41242</td>\n",
              "      <td>0.12170</td>\n",
              "      <td>0.34527</td>\n",
              "      <td>-0.044457</td>\n",
              "      <td>-0.49688</td>\n",
              "      <td>-0.17862</td>\n",
              "      <td>-0.00066</td>\n",
              "      <td>-0.656600</td>\n",
              "      <td>0.278430</td>\n",
              "      <td>-0.147670</td>\n",
              "      <td>-0.55677</td>\n",
              "      <td>0.14658</td>\n",
              "      <td>-0.00951</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>0.102040</td>\n",
              "      <td>-0.127920</td>\n",
              "      <td>-0.84430</td>\n",
              "      <td>-0.121810</td>\n",
              "      <td>-0.016801</td>\n",
              "      <td>-0.332790</td>\n",
              "      <td>-0.155200</td>\n",
              "      <td>-0.231310</td>\n",
              "      <td>-0.191810</td>\n",
              "      <td>-1.8823</td>\n",
              "      <td>-0.76746</td>\n",
              "      <td>0.099051</td>\n",
              "      <td>-0.421250</td>\n",
              "      <td>-0.19526</td>\n",
              "      <td>4.0071</td>\n",
              "      <td>-0.185940</td>\n",
              "      <td>-0.522870</td>\n",
              "      <td>-0.31681</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.17778</td>\n",
              "      <td>-0.158970</td>\n",
              "      <td>0.012041</td>\n",
              "      <td>-0.054223</td>\n",
              "      <td>-0.298710</td>\n",
              "      <td>-0.157490</td>\n",
              "      <td>-0.347580</td>\n",
              "      <td>-0.045637</td>\n",
              "      <td>-0.44251</td>\n",
              "      <td>0.187850</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>-0.184110</td>\n",
              "      <td>-0.115140</td>\n",
              "      <td>-0.78581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.236820</td>\n",
              "      <td>-0.16899</td>\n",
              "      <td>0.40951</td>\n",
              "      <td>0.63812</td>\n",
              "      <td>0.477090</td>\n",
              "      <td>-0.42852</td>\n",
              "      <td>-0.55641</td>\n",
              "      <td>-0.36400</td>\n",
              "      <td>-0.239380</td>\n",
              "      <td>0.130010</td>\n",
              "      <td>-0.063734</td>\n",
              "      <td>-0.39575</td>\n",
              "      <td>-0.48162</td>\n",
              "      <td>0.23291</td>\n",
              "      <td>0.090201</td>\n",
              "      <td>-0.133240</td>\n",
              "      <td>0.078639</td>\n",
              "      <td>-0.41634</td>\n",
              "      <td>-0.154280</td>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.488910</td>\n",
              "      <td>0.312260</td>\n",
              "      <td>-0.125200</td>\n",
              "      <td>-0.037512</td>\n",
              "      <td>-1.5179</td>\n",
              "      <td>0.12612</td>\n",
              "      <td>-0.024420</td>\n",
              "      <td>-0.042961</td>\n",
              "      <td>-0.28351</td>\n",
              "      <td>3.5416</td>\n",
              "      <td>-0.119560</td>\n",
              "      <td>-0.014533</td>\n",
              "      <td>-0.14990</td>\n",
              "      <td>0.218640</td>\n",
              "      <td>-0.334120</td>\n",
              "      <td>-0.13872</td>\n",
              "      <td>0.318060</td>\n",
              "      <td>0.703580</td>\n",
              "      <td>0.448580</td>\n",
              "      <td>-0.080262</td>\n",
              "      <td>0.630030</td>\n",
              "      <td>0.321110</td>\n",
              "      <td>-0.467650</td>\n",
              "      <td>0.22786</td>\n",
              "      <td>0.360340</td>\n",
              "      <td>-0.378180</td>\n",
              "      <td>-0.566570</td>\n",
              "      <td>0.044691</td>\n",
              "      <td>0.30392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.151640</td>\n",
              "      <td>0.301770</td>\n",
              "      <td>-0.16763</td>\n",
              "      <td>0.17684</td>\n",
              "      <td>0.31719</td>\n",
              "      <td>0.339730</td>\n",
              "      <td>-0.43478</td>\n",
              "      <td>-0.31086</td>\n",
              "      <td>-0.44999</td>\n",
              "      <td>-0.294860</td>\n",
              "      <td>0.166080</td>\n",
              "      <td>0.119630</td>\n",
              "      <td>-0.41328</td>\n",
              "      <td>-0.42353</td>\n",
              "      <td>0.59868</td>\n",
              "      <td>0.288250</td>\n",
              "      <td>-0.115470</td>\n",
              "      <td>-0.041848</td>\n",
              "      <td>-0.67989</td>\n",
              "      <td>-0.250630</td>\n",
              "      <td>0.184720</td>\n",
              "      <td>0.086876</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>0.015035</td>\n",
              "      <td>0.043474</td>\n",
              "      <td>-1.4671</td>\n",
              "      <td>-0.30384</td>\n",
              "      <td>-0.023441</td>\n",
              "      <td>0.305890</td>\n",
              "      <td>-0.21785</td>\n",
              "      <td>3.7460</td>\n",
              "      <td>0.004228</td>\n",
              "      <td>-0.184360</td>\n",
              "      <td>-0.46209</td>\n",
              "      <td>0.098329</td>\n",
              "      <td>-0.119070</td>\n",
              "      <td>0.23919</td>\n",
              "      <td>0.116100</td>\n",
              "      <td>0.417050</td>\n",
              "      <td>0.056763</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.068987</td>\n",
              "      <td>0.087939</td>\n",
              "      <td>-0.102850</td>\n",
              "      <td>-0.13931</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>-0.080803</td>\n",
              "      <td>-0.356520</td>\n",
              "      <td>0.016413</td>\n",
              "      <td>0.10216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.708530</td>\n",
              "      <td>0.570880</td>\n",
              "      <td>-0.47160</td>\n",
              "      <td>0.18048</td>\n",
              "      <td>0.54449</td>\n",
              "      <td>0.726030</td>\n",
              "      <td>0.18157</td>\n",
              "      <td>-0.52393</td>\n",
              "      <td>0.10381</td>\n",
              "      <td>-0.175660</td>\n",
              "      <td>0.078852</td>\n",
              "      <td>-0.362160</td>\n",
              "      <td>-0.11829</td>\n",
              "      <td>-0.83336</td>\n",
              "      <td>0.11917</td>\n",
              "      <td>-0.166050</td>\n",
              "      <td>0.061555</td>\n",
              "      <td>-0.012719</td>\n",
              "      <td>-0.56623</td>\n",
              "      <td>0.013616</td>\n",
              "      <td>0.228510</td>\n",
              "      <td>-0.143960</td>\n",
              "      <td>-0.067549</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>-0.236980</td>\n",
              "      <td>-1.7037</td>\n",
              "      <td>-0.86692</td>\n",
              "      <td>-0.267040</td>\n",
              "      <td>-0.258900</td>\n",
              "      <td>0.17670</td>\n",
              "      <td>3.8676</td>\n",
              "      <td>-0.161300</td>\n",
              "      <td>-0.132730</td>\n",
              "      <td>-0.68881</td>\n",
              "      <td>0.184440</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>-0.33874</td>\n",
              "      <td>-0.078956</td>\n",
              "      <td>0.241850</td>\n",
              "      <td>0.365760</td>\n",
              "      <td>-0.347270</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.38988</td>\n",
              "      <td>0.229020</td>\n",
              "      <td>-0.216170</td>\n",
              "      <td>-0.225620</td>\n",
              "      <td>-0.093918</td>\n",
              "      <td>-0.80375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.680470</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>0.30186</td>\n",
              "      <td>-0.17792</td>\n",
              "      <td>0.42962</td>\n",
              "      <td>0.032246</td>\n",
              "      <td>-0.41376</td>\n",
              "      <td>0.13228</td>\n",
              "      <td>-0.29847</td>\n",
              "      <td>-0.085253</td>\n",
              "      <td>0.171180</td>\n",
              "      <td>0.224190</td>\n",
              "      <td>-0.10046</td>\n",
              "      <td>-0.43653</td>\n",
              "      <td>0.33418</td>\n",
              "      <td>0.678460</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>-0.344480</td>\n",
              "      <td>-0.42785</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>0.559630</td>\n",
              "      <td>0.100320</td>\n",
              "      <td>0.186770</td>\n",
              "      <td>-0.268540</td>\n",
              "      <td>0.037334</td>\n",
              "      <td>-2.0932</td>\n",
              "      <td>0.22171</td>\n",
              "      <td>-0.398680</td>\n",
              "      <td>0.209120</td>\n",
              "      <td>-0.55725</td>\n",
              "      <td>3.8826</td>\n",
              "      <td>0.474660</td>\n",
              "      <td>-0.956580</td>\n",
              "      <td>-0.37788</td>\n",
              "      <td>0.208690</td>\n",
              "      <td>-0.327520</td>\n",
              "      <td>0.12751</td>\n",
              "      <td>0.088359</td>\n",
              "      <td>0.163510</td>\n",
              "      <td>-0.216340</td>\n",
              "      <td>-0.094375</td>\n",
              "      <td>0.018324</td>\n",
              "      <td>0.210480</td>\n",
              "      <td>-0.030880</td>\n",
              "      <td>-0.19722</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>-0.094340</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.064699</td>\n",
              "      <td>-0.26044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           1         2        3   ...        48        49       50\n",
              "0                                 ...                             \n",
              "the  0.418000  0.249680 -0.41242  ... -0.184110 -0.115140 -0.78581\n",
              ",    0.013441  0.236820 -0.16899  ... -0.566570  0.044691  0.30392\n",
              ".    0.151640  0.301770 -0.16763  ... -0.356520  0.016413  0.10216\n",
              "of   0.708530  0.570880 -0.47160  ... -0.225620 -0.093918 -0.80375\n",
              "to   0.680470 -0.039263  0.30186  ... -0.073297 -0.064699 -0.26044\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRpdrZoDuRPq",
        "colab_type": "text"
      },
      "source": [
        "### remove words which are not in embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY6gv67JuNhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cvocab =vocab.copy()\n",
        "for v in cvocab:\n",
        "  if v not in vectorsdf.index:\n",
        "    vocab.remove(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUQLz6tLvQsr",
        "colab_type": "text"
      },
      "source": [
        "### handle UNKOWN words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rvn8EKsukqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ids={}\n",
        "vocab_vectors=[]\n",
        "e_dim = 50\n",
        "\n",
        "# for i,v in enumerate(vocab):\n",
        "#   vocab_vectors.append(vectorsdf.loc[v].values.tolist())\n",
        "#   word_to_ids[v]=i\n",
        "for i,v in enumerate(vectorsdf.iterrows()):\n",
        "  word_to_ids[v[0]]=i\n",
        "\n",
        "vocab_vectors=vectorsdf.values\n",
        "\n",
        "vocab_vectors=np.array(vocab_vectors)\n",
        "word_to_ids[\"<UNK>\"]=len(vocab)\n",
        "word_to_ids[\"<PAD>\"]=0\n",
        "vocab_vectors = np.concatenate([vocab_vectors,np.ones((1,e_dim))],axis=0)\n",
        "vocab_vectors=np.concatenate([vocab_vectors,np.ones((1,e_dim))*20],axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoJMzMMb1P6n",
        "colab_type": "text"
      },
      "source": [
        "### prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtBtwppgwcn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(df,word2id):\n",
        "  new_dataset=[]\n",
        "  for r in df.iteritems():\n",
        "      s=[]\n",
        "      for w in r[1].split(\" \"):\n",
        "        if w in word2id:\n",
        "          s.append(word2id[w])\n",
        "        else:\n",
        "          s.append(word2id['<UNK>'])\n",
        "      new_dataset.append(s)    \n",
        "  return new_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piIpBY411SmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_dataset = convert_data(df['sentence'],word_to_ids)\n",
        "new_dataset = pad_sequences(new_dataset, maxlen=128, dtype='int32', padding='post', truncating='post', value=0.0)\n",
        "new_dataset = np.concatenate([new_dataset,df['polarity'].values.reshape(-1,1)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwlAKz-g2BHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = new_dataset[:int(len(new_dataset)*.8)]\n",
        "test = new_dataset[int(len(new_dataset)*.8):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So0NETcW2BfA",
        "colab_type": "text"
      },
      "source": [
        "### build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ6vmiktpgBy",
        "colab_type": "code",
        "outputId": "27c92aed-a9e7-4ae0-e06e-3f40b3f53b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Embedding(vocab_vectors.shape[0],\n",
        "                            50,\n",
        "                            embeddings_initializer=Constant(vocab_vectors),\n",
        "                            input_length=128,\n",
        "                            trainable=True))\n",
        "\n",
        "model.add(keras.layers.LSTM(32,activation=tf.nn.tanh))\n",
        "model.add(keras.layers.Dense(2,activation=tf.nn.softmax))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 128, 50)           20000100  \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 20,010,790\n",
            "Trainable params: 20,010,790\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1NM3wpU1HN4",
        "colab_type": "code",
        "outputId": "fd5471fd-329c-476b-bed3-fb958e338d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x = train[:,:-1],y=train[:,-1],batch_size=4000,epochs=30,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/30\n",
            "32000/32000 [==============================] - 4s 124us/sample - loss: 0.6930 - acc: 0.5131 - val_loss: 0.6926 - val_acc: 0.5170\n",
            "Epoch 2/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.6892 - acc: 0.5363 - val_loss: 0.6896 - val_acc: 0.5397\n",
            "Epoch 3/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.6857 - acc: 0.5557 - val_loss: 0.6867 - val_acc: 0.5530\n",
            "Epoch 4/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.6813 - acc: 0.5677 - val_loss: 0.6818 - val_acc: 0.5604\n",
            "Epoch 5/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.6725 - acc: 0.5877 - val_loss: 0.6697 - val_acc: 0.5888\n",
            "Epoch 6/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.6366 - acc: 0.6427 - val_loss: 0.6280 - val_acc: 0.6795\n",
            "Epoch 7/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.5990 - acc: 0.6956 - val_loss: 0.5859 - val_acc: 0.7075\n",
            "Epoch 8/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.5672 - acc: 0.7205 - val_loss: 0.5622 - val_acc: 0.7300\n",
            "Epoch 9/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.5384 - acc: 0.7448 - val_loss: 0.5408 - val_acc: 0.7449\n",
            "Epoch 10/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.5149 - acc: 0.7632 - val_loss: 0.5272 - val_acc: 0.7535\n",
            "Epoch 11/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.4984 - acc: 0.7730 - val_loss: 0.5160 - val_acc: 0.7601\n",
            "Epoch 12/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.4893 - acc: 0.7770 - val_loss: 0.5164 - val_acc: 0.7582\n",
            "Epoch 13/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.4680 - acc: 0.7904 - val_loss: 0.5182 - val_acc: 0.7606\n",
            "Epoch 14/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.4582 - acc: 0.7977 - val_loss: 0.5051 - val_acc: 0.7676\n",
            "Epoch 15/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.4369 - acc: 0.8104 - val_loss: 0.4852 - val_acc: 0.7825\n",
            "Epoch 16/30\n",
            "32000/32000 [==============================] - 2s 69us/sample - loss: 0.4185 - acc: 0.8225 - val_loss: 0.4805 - val_acc: 0.7861\n",
            "Epoch 17/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.4013 - acc: 0.8309 - val_loss: 0.4754 - val_acc: 0.7890\n",
            "Epoch 18/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.3870 - acc: 0.8419 - val_loss: 0.4721 - val_acc: 0.7911\n",
            "Epoch 19/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.3726 - acc: 0.8493 - val_loss: 0.4744 - val_acc: 0.7912\n",
            "Epoch 20/30\n",
            "32000/32000 [==============================] - 2s 66us/sample - loss: 0.3592 - acc: 0.8568 - val_loss: 0.4790 - val_acc: 0.7968\n",
            "Epoch 21/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.3496 - acc: 0.8622 - val_loss: 0.4710 - val_acc: 0.7990\n",
            "Epoch 22/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.3402 - acc: 0.8672 - val_loss: 0.4586 - val_acc: 0.7955\n",
            "Epoch 23/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.3283 - acc: 0.8730 - val_loss: 0.4627 - val_acc: 0.7956\n",
            "Epoch 24/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.3160 - acc: 0.8800 - val_loss: 0.4653 - val_acc: 0.8026\n",
            "Epoch 25/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.3015 - acc: 0.8882 - val_loss: 0.4690 - val_acc: 0.8024\n",
            "Epoch 26/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.2936 - acc: 0.8932 - val_loss: 0.4740 - val_acc: 0.8026\n",
            "Epoch 27/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.2900 - acc: 0.8931 - val_loss: 0.4742 - val_acc: 0.8035\n",
            "Epoch 28/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.2821 - acc: 0.8958 - val_loss: 0.4688 - val_acc: 0.8004\n",
            "Epoch 29/30\n",
            "32000/32000 [==============================] - 2s 68us/sample - loss: 0.2643 - acc: 0.9066 - val_loss: 0.4672 - val_acc: 0.8012\n",
            "Epoch 30/30\n",
            "32000/32000 [==============================] - 2s 67us/sample - loss: 0.2534 - acc: 0.9130 - val_loss: 0.4801 - val_acc: 0.8043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f27a01aa208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPd100DU2a1m",
        "colab_type": "code",
        "outputId": "ab205578-d501-4080-8058-74602898d76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "predicts = model.predict(train[:,:-1])\n",
        "evaluate(train[:,-1],predicts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([51475,  1645, 39211, 51475, 51475, 51021, 26773,  7784, 40901,\n",
              "       33471, 46286, 24964, 51475, 51475, 11268, 51475, 11268, 51475,\n",
              "       51475, 51475, 11268, 33514, 47844, 39002, 27407, 33514, 43300,\n",
              "       42118, 17699, 24565, 43877, 46395, 12199, 51475, 49453, 51475,\n",
              "       51475, 26608, 45030, 32702, 50970, 36536, 26562,  5672, 23960,\n",
              "       31028, 41316, 45137, 20395, 11268, 45137,  2726, 51475, 51475,\n",
              "       51475, 45137, 51037, 51417, 51475, 11268, 51475,  1519,  4962,\n",
              "       40392, 40555, 50970, 51475, 51475, 51475, 23740,  7784, 10113,\n",
              "       43877, 11893, 23170,  6598, 51475, 51475, 51475, 50970, 51475,\n",
              "        7784, 28580, 23170, 17699, 51475, 51475, 51475,  8521, 43877,\n",
              "       17699, 10682, 42658, 23170, 17699, 51475, 51475, 51475, 51475,\n",
              "       51475, 51475, 51475,  8830, 51475, 21227, 39901, 12329,  8296,\n",
              "       42656, 50970, 36536, 51475, 51475, 51475, 45137, 43454, 51475,\n",
              "       51475, 51475, 51475,  1927, 51058, 17383, 11582, 28058, 51475,\n",
              "       51475, 51475], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    }
  ]
}