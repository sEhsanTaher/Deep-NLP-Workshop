{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep nlp workshop.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sEhsanTaher/Deep-NLP-Workshop/blob/master/notebooks/deep_nlp_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAarcVP-4vXL",
        "colab_type": "text"
      },
      "source": [
        "# Mounting Google Drive to Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxE84wgf1x1G",
        "colab_type": "code",
        "outputId": "665c46fa-acc5-4a3d-f347-66ca3d08a889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8htyg0m2NIJ",
        "colab_type": "code",
        "outputId": "340c2230-e65c-461a-f02e-b92117cf4d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/'My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKwnmtBD4VWN",
        "colab_type": "text"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaQn-FUAs3qc",
        "colab_type": "code",
        "outputId": "85c473db-9492-4967-911e-bbe8911f4928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZdSxwLFtEn3",
        "colab_type": "code",
        "outputId": "ab20eebd-dfdd-4249-bfec-d46281d2cd5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Loading NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBNQ-YHttHfq",
        "colab_type": "code",
        "outputId": "207b44db-addc-424a-881d-1df318f28678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
        "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n",
        "tokenized_text=sent_tokenize(text)\n",
        "print(tokenized_text)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDAUg6JRw8qE",
        "colab_type": "code",
        "outputId": "2e2f4baa-96cb-4a5e-929a-ffd8fa113f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdINUN4tJTZ",
        "colab_type": "code",
        "outputId": "4336b365-2354-49f7-8c0c-2c0c460314e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbYrQgrAxC4v",
        "colab_type": "code",
        "outputId": "ee5d805f-5d7e-462f-8067-b6e0c2b5c36d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenized_word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yjlPyR2tXY4",
        "colab_type": "code",
        "outputId": "c66c03be-1cdf-4fc7-da2e-cbe1bd097a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'me', 'hers', 'themselves', 'yours', \"aren't\", 'she', 'off', 'does', 'all', 've', 'ain', \"you'll\", 'did', 'with', 'against', \"hasn't\", \"shouldn't\", 'under', 'of', 'ourselves', \"you've\", 'no', 'very', 'same', 'any', 'again', 'couldn', 'your', 'during', 'over', \"needn't\", \"wasn't\", 'haven', 'them', 'were', 'it', 'the', 'didn', 'is', 'he', 'that', 'mightn', 'each', 'here', 'his', 'just', 'himself', 'herself', 'you', 'whom', 'how', 'out', \"should've\", 'when', 'was', 'having', 'wouldn', 'above', \"it's\", \"you'd\", 'who', 'are', 'such', 'doesn', 'should', 'through', 'y', 'some', 'those', 'other', \"didn't\", 'and', 'own', 'hasn', 'its', 'into', 'most', 'on', 'ma', 'below', 'they', \"don't\", 'until', 'both', 'i', 'won', 'do', \"you're\", 'to', \"wouldn't\", 'or', 'there', 'from', 'then', 'but', 'be', 'am', 'once', 'as', 'shouldn', 'had', 'we', 's', 'not', \"haven't\", 'will', 'theirs', 'needn', \"won't\", 'only', 't', 'yourselves', 'have', \"she's\", 'which', 'because', 'up', 'shan', 'myself', 'where', 'for', 'don', 'a', 'after', 'than', 'by', \"doesn't\", 'o', 'itself', 'has', 'why', 'so', 'about', 'if', 'd', 'these', \"isn't\", 'wasn', 'in', 'been', 'at', 'isn', 'being', 'doing', 'ours', 'what', 'nor', 'our', 'few', 'can', 'further', 'my', 'before', 'now', \"couldn't\", 'an', 'mustn', \"that'll\", \"shan't\", 're', 'her', 'too', 'their', 'him', 'weren', 'this', 'down', 'm', \"mightn't\", 'yourself', 'aren', \"mustn't\", 'hadn', 'between', 'while', \"hadn't\", 'll', 'more', \"weren't\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuhgXCkJtdSa",
        "colab_type": "code",
        "outputId": "c73ca430-3f5b-436b-8139-07f15bd47dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "filtered_sent=[]\n",
        "for w in tokenized_word:\n",
        "    if w not in stop_words:\n",
        "        filtered_sent.append(w)\n",
        "print(\"Tokenized Sentence:\",tokenized_word)\n",
        "print(\"Filterd Sentence:\",filtered_sent)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized Sentence: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
            "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEXBVFH0xWs7",
        "colab_type": "code",
        "outputId": "ea9264ec-afd6-4253-c59c-9ef996383439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(filtered_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38rTY2V0tlIN",
        "colab_type": "code",
        "outputId": "e65f578f-f024-4945-b4c8-04569aa0bd53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_words=[]\n",
        "for w in filtered_sent:\n",
        "    stemmed_words.append(ps.stem(w))\n",
        "\n",
        "print(\"Filtered Sentence:\",filtered_sent)\n",
        "print(\"Stemmed Sentence:\",stemmed_words)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
            "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0NCqzjFuFDd",
        "colab_type": "code",
        "outputId": "f9ea1e29-46f9-4404-9465-ce2fb71e1979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Lexicon Normalization\n",
        "#performing stemming and Lemmatization\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stem = PorterStemmer()\n",
        "\n",
        "word = \"flying\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemmatized Word: fly\n",
            "Stemmed Word: fli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TazffC9juFU0",
        "colab_type": "code",
        "outputId": "50412ba0-7795-42a2-c274-154f946833c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "nltk.pos_tag(tokenized_word)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hello', 'NNP'),\n",
              " ('Mr.', 'NNP'),\n",
              " ('Smith', 'NNP'),\n",
              " (',', ','),\n",
              " ('how', 'WRB'),\n",
              " ('are', 'VBP'),\n",
              " ('you', 'PRP'),\n",
              " ('doing', 'VBG'),\n",
              " ('today', 'NN'),\n",
              " ('?', '.'),\n",
              " ('The', 'DT'),\n",
              " ('weather', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('great', 'JJ'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('city', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('awesome', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('sky', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('pinkish-blue', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('You', 'PRP'),\n",
              " ('should', 'MD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('eat', 'VB'),\n",
              " ('cardboard', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYaM7Ups4a6_",
        "colab_type": "text"
      },
      "source": [
        "# Stanford NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYuUG6_R0QQU",
        "colab_type": "code",
        "outputId": "58a148c6-ba7d-4ca5-91ca-ca2b0b0d320a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install stanfordnlp\n",
        "import stanfordnlp\n",
        "stanfordnlp.download('fa')   # This downloads the English models for the neural pipeline\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.17.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (42.0.2)\n",
            "Using the default treebank \"fa_seraji\" for language \"fa\".\n",
            "Would you like to download the models for: fa_seraji now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: fa_seraji\n",
            "Download location: /root/stanfordnlp_resources/fa_seraji_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 226M/226M [00:09<00:00, 22.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/fa_seraji_models.zip\n",
            "Extracting models file for: fa_seraji\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKggDFF60QrM",
        "colab_type": "code",
        "outputId": "5cf5c07c-22fc-43d6-d0e7-403790973e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "import stanfordnlp\n",
        "# nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
        "nlp = stanfordnlp.Pipeline(lang= 'fa', processors='tokenize,pos,depparse,mwt,lemma',tokenize_pretokenized=False, verbose=False)\n",
        "doc = nlp(\"رئیس سازمان برنامه و بودجه درباره درخواست نمایندگان مبنی بر کاهش قیمت بنزین در بودجه ۹۹ توضیحاتی ارائه کرد.\")\n",
        "for word in doc.sentences[0].words:\n",
        "  print(\"Word:{}\\tUPOS:{}\\tXPOS:{}\\tDEP:{}\\tLEMMA:{}\".format(word.text,word.upos,word.xpos,word.dependency_relation,word.lemma))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: gpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_tokenizer.pt', 'pretokenized': False, 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji.pretrain.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji.pretrain.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "---\n",
            "Loading: mwt\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_mwt_expander.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_lemmatizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "Done loading processors!\n",
            "---\n",
            "Word:رئیس\tUPOS:NOUN\tXPOS:N_SING\tDEP:nsubj\tLEMMA:رئیس\n",
            "Word:سازمان\tUPOS:NOUN\tXPOS:N_SING\tDEP:nmod:poss\tLEMMA:سازمان\n",
            "Word:برنامه\tUPOS:NOUN\tXPOS:N_SING\tDEP:nmod:poss\tLEMMA:برنامه\n",
            "Word:و\tUPOS:CCONJ\tXPOS:CON\tDEP:cc\tLEMMA:و\n",
            "Word:بودجه\tUPOS:NOUN\tXPOS:N_SING\tDEP:conj\tLEMMA:بودجه\n",
            "Word:درباره\tUPOS:ADP\tXPOS:P\tDEP:case\tLEMMA:درباره\n",
            "Word:درخواست\tUPOS:NOUN\tXPOS:N_SING\tDEP:nmod\tLEMMA:درخواست\n",
            "Word:نمایندگان\tUPOS:NOUN\tXPOS:N_PL\tDEP:nmod:poss\tLEMMA:نماینده\n",
            "Word:مبنی\tUPOS:ADP\tXPOS:P\tDEP:case\tLEMMA:مبنی\n",
            "Word:بر\tUPOS:ADP\tXPOS:P\tDEP:fixed\tLEMMA:بر\n",
            "Word:کاهش\tUPOS:NOUN\tXPOS:N_SING\tDEP:obl\tLEMMA:کاهش\n",
            "Word:قیمت\tUPOS:NOUN\tXPOS:N_SING\tDEP:nmod:poss\tLEMMA:قیمت\n",
            "Word:بنزین\tUPOS:NOUN\tXPOS:N_SING\tDEP:nmod:poss\tLEMMA:None\n",
            "Word:در\tUPOS:ADP\tXPOS:P\tDEP:case\tLEMMA:در\n",
            "Word:بودجه\tUPOS:NOUN\tXPOS:N_SING\tDEP:obl\tLEMMA:بودجه\n",
            "Word:۹۹\tUPOS:NUM\tXPOS:NUM\tDEP:nummod\tLEMMA:۹۹\n",
            "Word:توضیحاتی\tUPOS:NOUN\tXPOS:N_PL\tDEP:obj\tLEMMA:توضیح\n",
            "Word:ارائه\tUPOS:NOUN\tXPOS:N_SING\tDEP:compound:lvc\tLEMMA:ارائه\n",
            "Word:کرد\tUPOS:VERB\tXPOS:V_PA\tDEP:root\tLEMMA:کرد#کن\n",
            "Word:.\tUPOS:PUNCT\tXPOS:DELM\tDEP:punct\tLEMMA:.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7cEDsNN4hoe",
        "colab_type": "text"
      },
      "source": [
        "# Text Data Preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfm5vPTy2bNV",
        "colab_type": "code",
        "outputId": "89b51fca-0cda-45af-83ce-f20adf3f6594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sentiment.csv\")\n",
        "df.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I read that Jessie Matthews was approached and...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Yes, The Southern Star features a pretty forge...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>This is definatley one of the best stand-up sh...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>'Nobody knows anybody' is a conspiracy theory ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>CHANCES ARE is a charming romantic fantasy abo...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>I liked it! The plot was weird, Drew Barrymore...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>I've seen this movie twice with my teenagers w...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>Being a fan of cheesy horror movies, I saw thi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>Long before Tim LaHaye and Jerry B. Jenkins wo...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>If you're a T-Rex/Marc Bolan fan, I recommend ...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... polarity\n",
              "0           0  ...        1\n",
              "1           1  ...        0\n",
              "2           2  ...        1\n",
              "3           3  ...        0\n",
              "4           4  ...        1\n",
              "5           5  ...        1\n",
              "6           6  ...        1\n",
              "7           7  ...        0\n",
              "8           8  ...        1\n",
              "9           9  ...        1\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS-50mXW2bmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word2id(df):\n",
        "  vocab=set()\n",
        "  for i in df.iteritems():\n",
        "    words= i[1].split(' ')\n",
        "    vocab.update(words)\n",
        "  word2id= dict()\n",
        "  id2word= dict()\n",
        "  for i,word in enumerate(vocab):\n",
        "    word2id[word]=i+1\n",
        "    id2word[i+1] =word\n",
        "  return word2id,id2word,vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S34nxDtJ3ZUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2id,id2word,vocab = get_word2id(df['sentence'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he0O8xKp3bJ9",
        "colab_type": "code",
        "outputId": "5bafb6b4-7e9d-40aa-8f06-234c4f9a58e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(word2id[\"good\"])\n",
        "\n",
        "print(word2id[\"excellent\"])\n",
        "\n",
        "print(word2id[\"bad\"])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "246674\n",
            "42246\n",
            "96423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp4uh_zw3ZOJ",
        "colab_type": "code",
        "outputId": "55fc93cf-1a33-45dc-dde8-982fd38a84d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(id2word[332484])\n",
        "\n",
        "print(id2word[332485])\n",
        "\n",
        "print(id2word[332486])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyzed\n",
            "Wars\").\n",
            "expositional\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaiVKenJ3uRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(df,word2id):\n",
        "  new_dataset=[]\n",
        "  for r in df.iteritems():\n",
        "    new_dataset.append([word2id[w] for w in r[1].split(\" \")])\n",
        "  return new_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE1TvJ_930ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_dataset = convert_data(df['sentence'],word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2q-dd2g39h8",
        "colab_type": "code",
        "outputId": "8c7b4cb6-b067-4aff-d0d3-66f2713827db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(new_dataset[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[121335, 147362, 30869, 250578, 78908, 384942, 181084, 302101, 22920, 63272, 1716, 396966, 335977, 412997, 395144, 396879, 395144, 70977, 250578, 78908, 395144, 172494, 68834, 299801, 395662, 172494, 149480, 104160, 258898, 87484, 178009, 402218, 59215, 42426, 71454, 423139, 188619, 196030, 222873, 368717, 233232, 141055, 420232, 31960, 316403, 62773, 306099, 269270, 98846, 395144, 269270, 161364, 17327, 264092, 295235, 269270, 383199, 72937, 396879, 395144, 343993, 66263, 316085, 342326, 160366, 233232, 286818, 335977, 75558, 54199, 302101, 29526, 178009, 233219, 331761, 323927, 426418, 171851, 186738, 233232, 411628, 302101, 235996, 331761, 258898, 334686, 421245, 346724, 282011, 178009, 258898, 229140, 166848, 331761, 258898, 84334, 415683, 301459, 193450, 169337, 295273, 222252, 48996, 235126, 56149, 153156, 170568, 29111, 171203, 233232, 141055, 381696, 264092, 312600, 269270, 430222, 387178, 357215, 178754, 410838, 221905, 311771, 138384, 193121, 292233, 335977, 75558, 396879, 395144, 343993, 201509, 258898, 188398, 420232, 412997, 221905, 118863, 410838, 18730, 196030, 77975, 269270, 232583, 130784, 211510, 69360, 384942, 248724, 395144, 141055, 62382, 439463, 264092, 114581, 15290, 66263, 37934, 104160, 258898, 23804, 30869, 178754, 32075, 160519, 402218, 395144, 431191, 423139, 100034, 22555, 140226, 66263, 178009, 402218, 201509, 168805, 223164, 302101, 145257, 346406, 260, 319047, 227130, 316390, 355465, 328689, 371702, 121335, 222873, 178009, 176287, 52807, 258898, 412674, 431329, 104160, 335977, 75558, 211510, 69360, 316403, 299967, 146758, 129488, 258898, 428550, 116090, 298890, 372481, 201509, 401059, 241306, 395144, 258898, 37224, 120036, 247573, 25880, 342535, 137057, 264092, 195022, 22555, 406125, 97310, 421226, 395144, 269270, 71454, 396966, 66481, 381262, 116090, 386398, 409184, 45511, 369197, 258027, 30869, 69360, 201509, 304751, 71955, 71454, 178009, 360521, 264092, 148840, 102985, 121335, 62773, 201509, 269270, 245675, 6897, 302101, 395144, 141055, 375490, 121335, 255660, 31960, 386330, 434935, 191894, 229802, 116090, 45130, 158072, 353133, 362247, 323710, 104160, 426418, 335832, 201509, 345057, 122572, 146758, 395144, 141055, 134538, 170862, 315691, 395144, 30869, 330143, 121335, 111515, 77902, 396966, 412997, 302101, 69360, 270778, 157786, 163513, 388468, 395144, 258898, 80621, 264092, 166322, 421226, 258898, 293259, 65925, 48356, 269270, 145725, 104160, 178184, 59402, 266884, 439681, 412997, 395144, 423841, 193490, 395144, 258898, 294122, 293979, 184571, 342535, 116090, 434115, 413934, 104081, 201509, 90849, 372593, 264092, 75869, 424931, 150353, 215469, 258898, 212969, 396966, 22224, 54042, 129488, 172494, 45301, 388620, 302101, 233232, 98846, 323644, 152848, 48356, 337323, 154035, 48356, 269270, 389103, 253487, 421226, 381696, 264092, 148840, 99205, 147560, 146758, 104160, 112415, 396879, 395144, 343993, 201509, 112415, 369613, 309964, 338041, 258898, 388620, 361456, 193846, 223594, 258898, 187525, 304628, 306854, 317555, 168320, 326112, 379879, 321802, 2291, 1160, 395144, 258898, 388620, 171666, 282755, 287848, 331761, 335977, 75558, 45511, 6897, 104160, 301001, 235996, 30869, 143167, 69787, 48356, 153475, 48356, 168320, 330662, 302101, 168380, 185330, 264092, 193706, 258898, 180594, 302101, 47990, 211510, 369673, 112415, 396879, 395144, 343993, 330021, 48356, 269270, 284276, 42442, 244453, 42442, 165914, 42442, 280932, 409770, 170568, 29111, 233232, 316390, 103271]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st4pkury4XQi",
        "colab_type": "code",
        "outputId": "b1dab8ff-e1fe-4121-cea2-d02dc3e266b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(id2word[121335],id2word[147362],id2word[30869],id2word[250578])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I read that Jessie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po6cIg0x351z",
        "colab_type": "code",
        "outputId": "8ebd49e5-8aae-4f06-86cf-9452ddaa5a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(new_dataset[0]))\n",
        "print(len(new_dataset[1]))\n",
        "print(len(new_dataset[2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "444\n",
            "91\n",
            "85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBlozHOh7TaI",
        "colab_type": "code",
        "outputId": "100e2acb-3d77-4a41-efa3-b2fe789655dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhj0WbGP39DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_dataset = pad_sequences(new_dataset, maxlen=256, dtype='int32', padding='post', truncating='post', value=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vNRX0207XKE",
        "colab_type": "code",
        "outputId": "456ad273-b58c-4dc2-c983-2d1cf070bde8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(new_dataset[0]))\n",
        "print(len(new_dataset[1]))\n",
        "print(len(new_dataset[2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "256\n",
            "256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0plAcpE-7Yuz",
        "colab_type": "code",
        "outputId": "ac9765ad-d035-4a59-97d6-a7ca967f67f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(new_dataset[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[291647 116090 265246 158465 414028 269270 119172  31790 313453 385712\n",
            "  20307 331761  30869 223839 185980 378486 256060 182199 312114 119172\n",
            "  56149  22694 258898 129307 233232 141055  28705 302101 297347 245862\n",
            " 236968  82726 122467 396966 334763 314241 302101 387883 310921  48356\n",
            "  11661 381888 145725 395144 177266 104160 269270  23015 205097 111657\n",
            " 395144 283930  32912 125238 269270 309531 382745 369197  72613 236005\n",
            "  11661 175564 201079 104160  94882  65817 104160  63359 302101 212704\n",
            "   3706 302101 245862 372697 331761 132684 363465 302101 153156 235739\n",
            " 171131 396966 269270 389345  90882  75071 315960 258898  71190 104160\n",
            "    977      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcGfuTKy73p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "new_dataset = np.concatenate([new_dataset,df['polarity'].values.reshape(-1,1)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7lFX74u8KOE",
        "colab_type": "code",
        "outputId": "1a763433-bb42-4323-e750-041d87347218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(new_dataset[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[291647 116090 265246 158465 414028 269270 119172  31790 313453 385712\n",
            "  20307 331761  30869 223839 185980 378486 256060 182199 312114 119172\n",
            "  56149  22694 258898 129307 233232 141055  28705 302101 297347 245862\n",
            " 236968  82726 122467 396966 334763 314241 302101 387883 310921  48356\n",
            "  11661 381888 145725 395144 177266 104160 269270  23015 205097 111657\n",
            " 395144 283930  32912 125238 269270 309531 382745 369197  72613 236005\n",
            "  11661 175564 201079 104160  94882  65817 104160  63359 302101 212704\n",
            "   3706 302101 245862 372697 331761 132684 363465 302101 153156 235739\n",
            " 171131 396966 269270 389345  90882  75071 315960 258898  71190 104160\n",
            "    977      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      4      0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}